#!/usr/bin/env python3
"""
æ¤œç´¢å“è³ªæ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Task 3-3A-3: æ¤œç´¢å“è³ªæ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆ
- æ§˜ã€…ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã§ã®å“è³ªè©•ä¾¡
- ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æ¸¬å®š
- é–¢é€£æ€§ã‚¹ã‚³ã‚¢åˆ†æ
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
"""

import asyncio
import json
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent / "backend"))

from services.search_service import SearchService, SearchServiceError


class SearchQualityTester:
    """æ¤œç´¢å“è³ªãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.search_service = SearchService()
        self.test_results = []
        self.performance_metrics = []

    async def run_all_tests(self) -> Dict[str, Any]:
        """å…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print("ğŸ” æ¤œç´¢å“è³ªæ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("=" * 50)

        # 1. åŸºæœ¬æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        await self._test_basic_search()

        # 2. é–¢é€£æ€§ãƒ†ã‚¹ãƒˆ
        await self._test_relevance()

        # 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        await self._test_performance()

        # 4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
        await self._test_error_handling()

        # 5. å¤šæ§˜ãªã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ
        await self._test_diverse_queries()

        # 6. çµæœã®åˆ†æã¨å ±å‘Š
        return await self._generate_report()

    async def _test_basic_search(self):
        """åŸºæœ¬æ¤œç´¢æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ“‹ 1. åŸºæœ¬æ¤œç´¢æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
        print("-" * 30)

        basic_queries = [
            "machine learning",
            "Azure",
            "Python",
            "artificial intelligence",
            "cloud computing",
            "natural language processing"
        ]

        for query in basic_queries:
            try:
                start_time = time.time()
                result = await self.search_service.search_documents(
                    query=query,
                    top=5
                )
                end_time = time.time()

                response_time = (end_time - start_time) * 1000  # ms
                doc_count = len(result.get("documents", []))

                test_result = {
                    "test_type": "basic_search",
                    "query": query,
                    "success": True,
                    "document_count": doc_count,
                    "response_time_ms": response_time,
                    "has_results": doc_count > 0
                }

                self.test_results.append(test_result)
                self.performance_metrics.append({
                    "query": query,
                    "response_time_ms": response_time
                })

                status = "âœ…" if doc_count > 0 else "âš ï¸"
                print(f"{status} '{query}': {doc_count} docs, {response_time:.1f}ms")

            except Exception as e:
                test_result = {
                    "test_type": "basic_search",
                    "query": query,
                    "success": False,
                    "error": str(e)
                }
                self.test_results.append(test_result)
                print(f"âŒ '{query}': Error - {e}")

    async def _test_relevance(self):
        """é–¢é€£æ€§ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ¯ 2. é–¢é€£æ€§ãƒ†ã‚¹ãƒˆ")
        print("-" * 30)

        relevance_tests = [
            {
                "query": "machine learning algorithms",
                "expected_keywords": ["machine", "learning", "algorithm", "model"],
                "min_score": 1.0
            },
            {
                "query": "Azure AI services",
                "expected_keywords": ["azure", "ai", "artificial", "intelligence"],
                "min_score": 1.0
            },
            {
                "query": "Python programming best practices",
                "expected_keywords": ["python", "programming", "code", "practice"],
                "min_score": 1.0
            }
        ]

        for test_case in relevance_tests:
            try:
                result = await self.search_service.search_documents(
                    query=test_case["query"],
                    top=3
                )

                documents = result.get("documents", [])
                if not documents:
                    print(f"âš ï¸ '{test_case['query']}': No results found")
                    continue

                # é–¢é€£æ€§åˆ†æ
                relevance_scores = []
                keyword_matches = []

                for doc in documents:
                    score = doc.get("score", 0)
                    content = doc.get("document", {}).get("content", "").lower()
                    title = doc.get("document", {}).get("title", "").lower()
                    full_text = f"{title} {content}"

                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                    matches = sum(1 for keyword in test_case["expected_keywords"]
                                if keyword.lower() in full_text)
                    keyword_match_ratio = matches / len(test_case["expected_keywords"])

                    relevance_scores.append(score)
                    keyword_matches.append(keyword_match_ratio)

                avg_score = sum(relevance_scores) / len(relevance_scores)
                avg_keyword_match = sum(keyword_matches) / len(keyword_matches)

                test_result = {
                    "test_type": "relevance",
                    "query": test_case["query"],
                    "avg_score": avg_score,
                    "avg_keyword_match": avg_keyword_match,
                    "meets_min_score": avg_score >= test_case["min_score"],
                    "document_count": len(documents)
                }

                self.test_results.append(test_result)

                status = "âœ…" if avg_score >= test_case["min_score"] else "âš ï¸"
                print(f"{status} '{test_case['query']}': Score {avg_score:.2f}, Keywords {avg_keyword_match:.1%}")

            except Exception as e:
                print(f"âŒ '{test_case['query']}': Error - {e}")

    async def _test_performance(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        print("\nâš¡ 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ")
        print("-" * 30)

        # åŒæ™‚å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
        concurrent_queries = [
            "AI", "ML", "Azure", "Python", "cloud", "data", "search", "algorithm"
        ]

        print("åŒæ™‚å®Ÿè¡Œãƒ†ã‚¹ãƒˆ (8ã‚¯ã‚¨ãƒªä¸¦åˆ—)...")
        start_time = time.time()

        tasks = [
            self.search_service.search_documents(query=query, top=3)
            for query in concurrent_queries
        ]

        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            end_time = time.time()

            total_time = (end_time - start_time) * 1000
            successful_queries = sum(1 for r in results if not isinstance(r, Exception))

            print(f"âœ… ä¸¦åˆ—å®Ÿè¡Œ: {successful_queries}/{len(concurrent_queries)} æˆåŠŸ, {total_time:.1f}ms")

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“åˆ†æ
            if self.performance_metrics:
                response_times = [m["response_time_ms"] for m in self.performance_metrics]
                avg_time = sum(response_times) / len(response_times)
                max_time = max(response_times)
                min_time = min(response_times)

                print(f"ğŸ“Š ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“çµ±è¨ˆ:")
                print(f"   å¹³å‡: {avg_time:.1f}ms")
                print(f"   æœ€å¤§: {max_time:.1f}ms")
                print(f"   æœ€å°: {min_time:.1f}ms")

                performance_result = {
                    "test_type": "performance",
                    "concurrent_success_rate": successful_queries / len(concurrent_queries),
                    "avg_response_time_ms": avg_time,
                    "max_response_time_ms": max_time,
                    "min_response_time_ms": min_time,
                    "total_concurrent_time_ms": total_time
                }
                self.test_results.append(performance_result)

        except Exception as e:
            print(f"âŒ ä¸¦åˆ—å®Ÿè¡Œãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

    async def _test_error_handling(self):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ›¡ï¸ 4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ")
        print("-" * 30)

        error_test_cases = [
            {"query": "", "description": "ç©ºã‚¯ã‚¨ãƒª"},
            {"query": "a" * 1000, "description": "è¶…é•·ã‚¯ã‚¨ãƒª"},
            {"query": "!@#$%^&*()", "description": "ç‰¹æ®Šæ–‡å­—"},
            {"query": "SELECT * FROM users", "description": "SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³è©¦è¡Œ"}
        ]

        for test_case in error_test_cases:
            try:
                result = await self.search_service.search_documents(
                    query=test_case["query"],
                    top=5
                )

                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãªã‹ã£ãŸå ´åˆ
                doc_count = len(result.get("documents", []))
                print(f"âœ… {test_case['description']}: {doc_count} docs (æ­£å¸¸å‡¦ç†)")

                test_result = {
                    "test_type": "error_handling",
                    "description": test_case["description"],
                    "query": test_case["query"][:50] + "..." if len(test_case["query"]) > 50 else test_case["query"],
                    "handled_gracefully": True,
                    "document_count": doc_count
                }
                self.test_results.append(test_result)

            except SearchServiceError as e:
                print(f"âœ… {test_case['description']}: é©åˆ‡ã«ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚° - {e}")
                test_result = {
                    "test_type": "error_handling",
                    "description": test_case["description"],
                    "handled_gracefully": True,
                    "error_type": "SearchServiceError"
                }
                self.test_results.append(test_result)

            except Exception as e:
                print(f"âš ï¸ {test_case['description']}: äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ - {e}")
                test_result = {
                    "test_type": "error_handling",
                    "description": test_case["description"],
                    "handled_gracefully": False,
                    "error_type": type(e).__name__,
                    "error_message": str(e)
                }
                self.test_results.append(test_result)

    async def _test_diverse_queries(self):
        """å¤šæ§˜ãªã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ"""
        print("\nğŸŒ 5. å¤šæ§˜ãªã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ")
        print("-" * 30)

        diverse_queries = [
            # æŠ€è¡“ç”¨èª
            {"query": "neural networks", "category": "technical"},
            {"query": "deep learning", "category": "technical"},
            {"query": "transformer architecture", "category": "technical"},

            # æ—¥æœ¬èªï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
            {"query": "æ©Ÿæ¢°å­¦ç¿’", "category": "japanese"},
            {"query": "äººå·¥çŸ¥èƒ½", "category": "japanese"},

            # è¤‡åˆã‚¯ã‚¨ãƒª
            {"query": "Azure machine learning service", "category": "compound"},
            {"query": "Python data science libraries", "category": "compound"},

            # çŸ­ã„ã‚¯ã‚¨ãƒª
            {"query": "AI", "category": "short"},
            {"query": "ML", "category": "short"},

            # é•·ã„ã‚¯ã‚¨ãƒª
            {"query": "how to implement machine learning algorithms in Python for data analysis", "category": "long"}
        ]

        category_results = {}

        for test_case in diverse_queries:
            try:
                start_time = time.time()
                result = await self.search_service.search_documents(
                    query=test_case["query"],
                    top=5
                )
                end_time = time.time()

                response_time = (end_time - start_time) * 1000
                doc_count = len(result.get("documents", []))

                category = test_case["category"]
                if category not in category_results:
                    category_results[category] = []

                category_results[category].append({
                    "query": test_case["query"],
                    "document_count": doc_count,
                    "response_time_ms": response_time
                })

                status = "âœ…" if doc_count > 0 else "âš ï¸"
                print(f"{status} [{category}] '{test_case['query']}': {doc_count} docs")

            except Exception as e:
                print(f"âŒ [{test_case['category']}] '{test_case['query']}': Error - {e}")

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        print("\nğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ:")
        for category, results in category_results.items():
            if results:
                avg_docs = sum(r["document_count"] for r in results) / len(results)
                avg_time = sum(r["response_time_ms"] for r in results) / len(results)
                success_rate = sum(1 for r in results if r["document_count"] > 0) / len(results)

                print(f"   {category}: å¹³å‡{avg_docs:.1f}docs, {avg_time:.1f}ms, æˆåŠŸç‡{success_rate:.1%}")

                category_result = {
                    "test_type": "diverse_queries",
                    "category": category,
                    "avg_document_count": avg_docs,
                    "avg_response_time_ms": avg_time,
                    "success_rate": success_rate,
                    "query_count": len(results)
                }
                self.test_results.append(category_result)

    async def _generate_report(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\nğŸ“‹ 6. ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 50)

        # å…¨ä½“çµ±è¨ˆ
        total_tests = len(self.test_results)
        successful_tests = sum(1 for r in self.test_results
                             if r.get("success", True) and not r.get("error"))

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        if self.performance_metrics:
            response_times = [m["response_time_ms"] for m in self.performance_metrics]
            avg_response_time = sum(response_times) / len(response_times)
        else:
            avg_response_time = 0

        # æ¤œç´¢å“è³ªçµ±è¨ˆ
        relevance_tests = [r for r in self.test_results if r.get("test_type") == "relevance"]
        if relevance_tests:
            avg_relevance_score = sum(r.get("avg_score", 0) for r in relevance_tests) / len(relevance_tests)
            avg_keyword_match = sum(r.get("avg_keyword_match", 0) for r in relevance_tests) / len(relevance_tests)
        else:
            avg_relevance_score = 0
            avg_keyword_match = 0

        report = {
            "test_summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "success_rate": successful_tests / total_tests if total_tests > 0 else 0,
                "test_timestamp": datetime.now().isoformat()
            },
            "performance_metrics": {
                "avg_response_time_ms": avg_response_time,
                "total_queries_tested": len(self.performance_metrics)
            },
            "quality_metrics": {
                "avg_relevance_score": avg_relevance_score,
                "avg_keyword_match_rate": avg_keyword_match,
                "relevance_tests_count": len(relevance_tests)
            },
            "detailed_results": self.test_results
        }

        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        print(f"âœ… ç·åˆæˆåŠŸç‡: {report['test_summary']['success_rate']:.1%}")
        print(f"âš¡ å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: {avg_response_time:.1f}ms")
        print(f"ğŸ¯ å¹³å‡é–¢é€£æ€§ã‚¹ã‚³ã‚¢: {avg_relevance_score:.2f}")
        print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒç‡: {avg_keyword_match:.1%}")

        # æ¨å¥¨äº‹é …
        print("\nğŸ’¡ æ¨å¥¨äº‹é …:")
        if avg_response_time > 5000:
            print("   âš ï¸ ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒ5ç§’ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        if avg_relevance_score < 2.0:
            print("   âš ï¸ é–¢é€£æ€§ã‚¹ã‚³ã‚¢ãŒä½ã‚ã§ã™ã€‚æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®èª¿æ•´ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        if avg_keyword_match < 0.5:
            print("   âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒç‡ãŒä½ã‚ã§ã™ã€‚ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å“è³ªå‘ä¸Šã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")

        if (avg_response_time <= 5000 and avg_relevance_score >= 2.0 and avg_keyword_match >= 0.5):
            print("   âœ… æ¤œç´¢å“è³ªã¯è‰¯å¥½ã§ã™ï¼")

        return report


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        tester = SearchQualityTester()
        report = await tester.run_all_tests()

        # ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        report_file = Path("test_results") / f"search_quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_file.parent.mkdir(exist_ok=True)

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {report_file}")
        print("\nğŸ‰ æ¤œç´¢å“è³ªæ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")

        return report

    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return None


if __name__ == "__main__":
    asyncio.run(main())
