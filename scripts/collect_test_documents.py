#!/usr/bin/env python3
"""
ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è»½é‡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰RAGç”¨ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åé›†
"""

import os
import json
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Any
import requests
from tqdm import tqdm
import pandas as pd

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TestDocumentCollector:
    """ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåé›†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, output_dir: str = "test_documents"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        (self.output_dir / "wikipedia").mkdir(exist_ok=True)
        (self.output_dir / "qa_datasets").mkdir(exist_ok=True)
        (self.output_dir / "technical_docs").mkdir(exist_ok=True)
        (self.output_dir / "sample_pdfs").mkdir(exist_ok=True)

    def collect_wikipedia_passages(self, limit: int = 20) -> None:
        """è»½é‡ãªWikipediaã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
        logger.info(f"Wikipediaã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆä¸­... (ä»¶æ•°: {limit})")

        # å®Ÿç”¨çš„ãªWikipediaã‚¹ã‚¿ã‚¤ãƒ«ã®æ–‡æ›¸ï¼ˆAIãƒ»æŠ€è¡“é–¢é€£ï¼‰
        wiki_samples = [
            {
                "id": "wiki_ai_001",
                "title": "äººå·¥çŸ¥èƒ½",
                "content": "äººå·¥çŸ¥èƒ½ï¼ˆã˜ã‚“ã“ã†ã¡ã®ã†ã€è‹±: artificial intelligenceã€AIï¼‰ã¨ã¯ã€äººé–“ã®çŸ¥çš„èƒ½åŠ›ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ä¸Šã§å®Ÿç¾ã™ã‚‹æŠ€è¡“ã®ç·ç§°ã§ã‚ã‚‹ã€‚æ©Ÿæ¢°å­¦ç¿’ã€æ·±å±¤å­¦ç¿’ã€è‡ªç„¶è¨€èªå‡¦ç†ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ãªã©ã®åˆ†é‡ãŒå«ã¾ã‚Œã‚‹ã€‚è¿‘å¹´ã§ã¯å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®ç™ºå±•ã«ã‚ˆã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã€ç¿»è¨³ã€è¦ç´„ãªã©ã®ã‚¿ã‚¹ã‚¯ã§äººé–“ãƒ¬ãƒ™ãƒ«ã®æ€§èƒ½ã‚’é”æˆã—ã¦ã„ã‚‹ã€‚ä¸»è¦ãªå¿œç”¨åˆ†é‡ã«ã¯ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€è‡ªå‹•ç¿»è¨³ã€ç”»åƒèªè­˜ã€éŸ³å£°èªè­˜ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ãªã©ãŒã‚ã‚‹ã€‚",
                "source": "wikipedia_sample",
                "type": "wikipedia_passage"
            },
            {
                "id": "wiki_ml_002",
                "title": "æ©Ÿæ¢°å­¦ç¿’",
                "content": "æ©Ÿæ¢°å­¦ç¿’ï¼ˆãã‹ã„ãŒãã—ã‚…ã†ã€è‹±: machine learningã€MLï¼‰ã¯ã€äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹æŠ€è¡“ã§ã‚ã‚‹ã€‚æ•™å¸«ã‚ã‚Šå­¦ç¿’ã€æ•™å¸«ãªã—å­¦ç¿’ã€å¼·åŒ–å­¦ç¿’ã®3ã¤ã®ä¸»è¦ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚‹ã€‚ä»£è¡¨çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¯ç·šå½¢å›å¸°ã€æ±ºå®šæœ¨ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãªã©ãŒã‚ã‚‹ã€‚è¿‘å¹´ã¯ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®ç™ºå±•ã«ã‚ˆã‚Šã€ç”»åƒèªè­˜ã€è‡ªç„¶è¨€èªå‡¦ç†ã€éŸ³å£°èªè­˜ãªã©ã®åˆ†é‡ã§å¤§å¹…ãªæ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚",
                "source": "wikipedia_sample",
                "type": "wikipedia_passage"
            },
            {
                "id": "wiki_nlp_003",
                "title": "è‡ªç„¶è¨€èªå‡¦ç†",
                "content": "è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆã—ãœã‚“ã’ã‚“ã”ã—ã‚‡ã‚Šã€è‹±: natural language processingã€NLPï¼‰ã¯ã€äººé–“ãŒæ—¥å¸¸çš„ã«ä½¿ã£ã¦ã„ã‚‹è‡ªç„¶è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«å‡¦ç†ã•ã›ã‚‹æŠ€è¡“ã§ã‚ã‚‹ã€‚å½¢æ…‹ç´ è§£æã€æ§‹æ–‡è§£æã€æ„å‘³è§£æã€æ–‡æ›¸åˆ†é¡ã€æ©Ÿæ¢°ç¿»è¨³ã€è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ãªã©ã®å¿œç”¨ãŒã‚ã‚‹ã€‚è¿‘å¹´ã¯Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åŸºç›¤ã¨ã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ãŒä¸»æµã¨ãªã£ã¦ã„ã‚‹ã€‚BERTã€GPTã€T5ãªã©ã®ãƒ¢ãƒ‡ãƒ«ãŒæ§˜ã€…ãªNLPã‚¿ã‚¹ã‚¯ã§é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚",
                "source": "wikipedia_sample",
                "type": "wikipedia_passage"
            },
            {
                "id": "wiki_cloud_004",
                "title": "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°",
                "content": "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼ˆè‹±: cloud computingï¼‰ã¯ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆçµŒç”±ã§ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒªã‚½ãƒ¼ã‚¹ã‚’æä¾›ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚IaaSï¼ˆInfrastructure as a Serviceï¼‰ã€PaaSï¼ˆPlatform as a Serviceï¼‰ã€SaaSï¼ˆSoftware as a Serviceï¼‰ã®3ã¤ã®ä¸»è¦ãªã‚µãƒ¼ãƒ“ã‚¹å½¢æ…‹ãŒã‚ã‚‹ã€‚ä¸»è¦ãªã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«ã¯ã€Amazon Web Servicesï¼ˆAWSï¼‰ã€Microsoft Azureã€Google Cloud Platformï¼ˆGCPï¼‰ãªã©ãŒã‚ã‚‹ã€‚ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã€ã‚³ã‚¹ãƒˆåŠ¹ç‡ã€å¯ç”¨æ€§ã®å‘ä¸ŠãŒä¸»ãªãƒ¡ãƒªãƒƒãƒˆã¨ã—ã¦æŒ™ã’ã‚‰ã‚Œã‚‹ã€‚",
                "source": "wikipedia_sample",
                "type": "wikipedia_passage"
            },
            {
                "id": "wiki_azure_005",
                "title": "Microsoft Azure",
                "content": "Microsoft Azureï¼ˆãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆ ã‚¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰ã¯ã€ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆãŒæä¾›ã™ã‚‹ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã‚ã‚‹ã€‚ä»®æƒ³ãƒã‚·ãƒ³ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€AIãƒ»æ©Ÿæ¢°å­¦ç¿’ã‚µãƒ¼ãƒ“ã‚¹ã€IoTã‚µãƒ¼ãƒ“ã‚¹ãªã©200ä»¥ä¸Šã®ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã¦ã„ã‚‹ã€‚Azure OpenAI Serviceã€Azure AI Searchã€Azure Cognitive Servicesãªã©ã€AIé–¢é€£ã‚µãƒ¼ãƒ“ã‚¹ãŒå……å®Ÿã—ã¦ã„ã‚‹ã€‚ä¸–ç•Œ60ä»¥ä¸Šã®ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã§ã‚µãƒ¼ãƒ“ã‚¹ã‚’å±•é–‹ã—ã€ä¼æ¥­ã®ãƒ‡ã‚¸ã‚¿ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ”¯æ´ã—ã¦ã„ã‚‹ã€‚",
                "source": "wikipedia_sample",
                "type": "wikipedia_passage"
            }
        ]

        # å¿…è¦ã«å¿œã˜ã¦ä»¶æ•°ã‚’èª¿æ•´
        wiki_samples = wiki_samples[:min(limit, len(wiki_samples))]

        # JSONä¿å­˜
        output_file = self.output_dir / "wikipedia" / "wiki_sample_passages.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(wiki_samples, f, ensure_ascii=False, indent=2)

        # å€‹åˆ¥ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        for doc in tqdm(wiki_samples, desc="Wikipediaãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"):
            text_file = self.output_dir / "wikipedia" / f"{doc['id']}.txt"
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(f"# {doc['title']}\n\n{doc['content']}")

        logger.info(f"âœ… Wikipediaã‚µãƒ³ãƒ—ãƒ« {len(wiki_samples)}ä»¶ã‚’ä½œæˆå®Œäº†")

    def collect_squad_dataset(self, limit: int = 30) -> None:
        """è»½é‡ãªSQuADã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
        logger.info(f"SQuADã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆä¸­... (ä»¶æ•°: {limit})")

        # å®Ÿç”¨çš„ãªQ&Aã‚µãƒ³ãƒ—ãƒ«ï¼ˆæŠ€è¡“ãƒ»ãƒ“ã‚¸ãƒã‚¹é–¢é€£ï¼‰
        squad_samples = [
            {
                "id": "squad_tech_001",
                "title": "Azure AI Search",
                "content": "Azure AI Searchã¯ã€MicrosoftãŒæä¾›ã™ã‚‹ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€å¤§è¦æ¨¡ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’é«˜é€Ÿã«æ¤œç´¢ã§ãã¾ã™ã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã€ã‚¯ã‚¨ãƒªå‡¦ç†ã€çµæœãƒ©ãƒ³ã‚­ãƒ³ã‚°ãªã©ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã€æ¤œç´¢ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æ§‹ç¯‰ã‚’æ”¯æ´ã—ã¾ã™ã€‚",
                "question": "Azure AI Searchã¯ã©ã®ã‚ˆã†ãªæ¤œç´¢æ–¹å¼ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã‹ï¼Ÿ",
                "answers": {"text": ["ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢"], "answer_start": [50]},
                "source": "squad_sample",
                "type": "qa_context"
            },
            {
                "id": "squad_tech_002",
                "title": "RAGï¼ˆRetrieval-Augmented Generationï¼‰",
                "content": "RAGã¯æ¤œç´¢æ‹¡å¼µç”Ÿæˆã¨å‘¼ã°ã‚Œã‚‹æ‰‹æ³•ã§ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®å›ç­”ç”Ÿæˆæ™‚ã«å¤–éƒ¨ã®çŸ¥è­˜æºã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ã—ã¦æ´»ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ãªã„æœ€æ–°æƒ…å ±ã‚„å°‚é–€çŸ¥è­˜ã‚’å›ç­”ã«åæ˜ ã§ãã¾ã™ã€‚RAGã‚·ã‚¹ãƒ†ãƒ ã¯ã€æ–‡æ›¸ã®å–ã‚Šè¾¼ã¿ã€ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã€æ¤œç´¢ã€ç”Ÿæˆã®4ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã§æ§‹æˆã•ã‚Œã¾ã™ã€‚",
                "question": "RAGã‚·ã‚¹ãƒ†ãƒ ã¯ä½•ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã§æ§‹æˆã•ã‚Œã¾ã™ã‹ï¼Ÿ",
                "answers": {"text": ["4ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—"], "answer_start": [120]},
                "source": "squad_sample",
                "type": "qa_context"
            },
            {
                "id": "squad_biz_003",
                "title": "ãƒ‡ã‚¸ã‚¿ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³",
                "content": "ãƒ‡ã‚¸ã‚¿ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆDXï¼‰ã¯ã€ãƒ‡ã‚¸ã‚¿ãƒ«æŠ€è¡“ã‚’æ´»ç”¨ã—ã¦ä¼æ¥­ã®æ¥­å‹™ãƒ—ãƒ­ã‚»ã‚¹ã€çµ„ç¹”æ–‡åŒ–ã€é¡§å®¢ä½“é¨“ã‚’æ ¹æœ¬çš„ã«å¤‰é©ã™ã‚‹ã“ã¨ã§ã™ã€‚ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã€AIã€IoTã€ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿ãªã©ã®æŠ€è¡“ã‚’çµ„ã¿åˆã‚ã›ã¦ã€æ–°ã—ã„ãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«ã®å‰µå‡ºã‚„ç«¶äº‰å„ªä½æ€§ã®ç¢ºç«‹ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚",
                "question": "DXã§æ´»ç”¨ã•ã‚Œã‚‹ä¸»è¦ãªæŠ€è¡“ã«ã¯ã©ã®ã‚ˆã†ãªã‚‚ã®ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
                "answers": {"text": ["ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã€AIã€IoTã€ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿"], "answer_start": [80]},
                "source": "squad_sample",
                "type": "qa_context"
            }
        ]

        # å¿…è¦ã«å¿œã˜ã¦ä»¶æ•°ã‚’èª¿æ•´
        squad_samples = squad_samples[:min(limit, len(squad_samples))]

        # JSONä¿å­˜
        output_file = self.output_dir / "qa_datasets" / "squad_contexts.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(squad_samples, f, ensure_ascii=False, indent=2)

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        for doc in tqdm(squad_samples, desc="SQuADãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"):
            text_file = self.output_dir / "qa_datasets" / f"{doc['id']}_context.txt"
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(f"# {doc['title']}\n\n{doc['content']}\n\n")
                f.write(f"## Sample Question\n{doc['question']}")

        logger.info(f"âœ… SQuADã‚µãƒ³ãƒ—ãƒ« {len(squad_samples)}ä»¶ã‚’ä½œæˆå®Œäº†")

    def collect_technical_samples(self) -> None:
        """æŠ€è¡“æ–‡æ›¸ã‚µãƒ³ãƒ—ãƒ«ä½œæˆ"""
        logger.info("æŠ€è¡“æ–‡æ›¸ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆä¸­...")

        # Azure AIé–¢é€£æŠ€è¡“æ–‡æ›¸ã‚µãƒ³ãƒ—ãƒ«
        azure_ai_doc = """# Azure AI Services æ¦‚è¦

## Azure OpenAI Service
Azure OpenAI Serviceã¯ã€OpenAIã®å¼·åŠ›ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’Azureã‚¯ãƒ©ã‚¦ãƒ‰ä¸Šã§åˆ©ç”¨ã§ãã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚

### ä¸»è¦æ©Ÿèƒ½
- GPT-4, GPT-3.5-turbo ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨
- ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã€è¦ç´„ã€ç¿»è¨³
- ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã¨ãƒ‡ãƒãƒƒã‚°æ”¯æ´
- ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆæ§‹ç¯‰

### æ–™é‡‘ä½“ç³»
- ãƒˆãƒ¼ã‚¯ãƒ³ãƒ™ãƒ¼ã‚¹ã®å¾“é‡èª²é‡‘åˆ¶
- ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ç•°ãªã‚‹æ–™é‡‘è¨­å®š
- ç„¡æ–™æ : æœˆé¡$200ç›¸å½“ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆ

## Azure AI Search
Azure AI Searchã¯ã€ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã¨ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚

### æ¤œç´¢æ©Ÿèƒ½
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆBM25ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
- ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼ˆãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦ï¼‰
- ãƒ•ã‚¡ã‚»ãƒƒãƒˆæ¤œç´¢ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
- è‡ªå‹•è£œå®Œã¨ã‚¹ãƒšãƒ«ä¿®æ­£

### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­è¨ˆ
- ã‚¹ã‚­ãƒ¼ãƒå®šç¾©ï¼ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã€ãƒ‡ãƒ¼ã‚¿å‹ï¼‰
- ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼è¨­å®šï¼ˆè¨€èªåˆ¥ï¼‰
- ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
- ã‚·ãƒãƒ‹ãƒ ãƒãƒƒãƒ—

## RAGï¼ˆRetrieval-Augmented Generationï¼‰
RAGã¯æ¤œç´¢æ‹¡å¼µç”Ÿæˆã¨å‘¼ã°ã‚Œã€å¤–éƒ¨çŸ¥è­˜æºã‚’æ´»ç”¨ã—ã¦LLMã®å›ç­”ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã§ã™ã€‚

### RAGã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–ã‚Šè¾¼ã¿ãƒ»ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
2. ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç™»éŒ²
3. ã‚¯ã‚¨ãƒªå®Ÿè¡Œãƒ»é–¢é€£æ–‡æ›¸æ¤œç´¢
4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
5. LLMã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆ

### å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ
- ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®æœ€é©åŒ–ï¼ˆ500-1000æ–‡å­—ï¼‰
- ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—è¨­å®šï¼ˆ10-20%ï¼‰
- æ¤œç´¢ç²¾åº¦å‘ä¸Šï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼‰
- å¼•ç”¨æƒ…å ±ã®ä»˜ä¸
"""

        # Pythoné–‹ç™ºã‚¬ã‚¤ãƒ‰ã‚µãƒ³ãƒ—ãƒ«
        python_guide_doc = """# Pythoné–‹ç™ºãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

## ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„

### PEP 8æº–æ‹ 
- ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ: ã‚¹ãƒšãƒ¼ã‚¹4ã¤
- è¡Œé•·: 79æ–‡å­—ä»¥å†…
- å‘½åè¦å‰‡: snake_caseï¼ˆå¤‰æ•°ãƒ»é–¢æ•°ï¼‰ã€PascalCaseï¼ˆã‚¯ãƒ©ã‚¹ï¼‰

### å‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
```python
def calculate_score(items: List[Dict[str, Any]]) -> float:
    total = sum(item.get('score', 0) for item in items)
    return total / len(items) if items else 0.0
```

## ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

### pytestä½¿ç”¨
- ãƒ†ã‚¹ãƒˆé–¢æ•°å: `test_` ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
- ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£æ´»ç”¨
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒ†ã‚¹ãƒˆ
- ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®š

### ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ä¾‹
```python
import pytest
from myapp.calculator import Calculator

class TestCalculator:
    def test_add_positive_numbers(self):
        calc = Calculator()
        result = calc.add(2, 3)
        assert result == 5

    @pytest.mark.parametrize("a,b,expected", [
        (1, 2, 3),
        (-1, 1, 0),
        (0, 0, 0),
    ])
    def test_add_various_inputs(self, a, b, expected):
        calc = Calculator()
        assert calc.add(a, b) == expected
```

## éåŒæœŸãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°

### asyncioåŸºæœ¬
- `async def` ã§éåŒæœŸé–¢æ•°å®šç¾©
- `await` ã§éåŒæœŸå‡¦ç†å¾…æ©Ÿ
- `asyncio.gather()` ã§ä¸¦è¡Œå®Ÿè¡Œ

### FastAPIçµ±åˆ
```python
from fastapi import FastAPI
import httpx

app = FastAPI()

@app.get("/search")
async def search_documents(query: str):
    async with httpx.AsyncClient() as client:
        response = await client.get(f"https://api.example.com/search?q={query}")
        return response.json()
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
- `cProfile` ã§ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
- `memory_profiler` ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–
- `py-spy` ã§æœ¬ç•ªç’°å¢ƒãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

### æœ€é©åŒ–æ‰‹æ³•
- ãƒªã‚¹ãƒˆå†…åŒ…è¡¨è¨˜ã®æ´»ç”¨
- ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ä½¿ç”¨ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
- `functools.lru_cache` ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªæœ€é©åŒ–
"""

        # FAQæ–‡æ›¸ã‚µãƒ³ãƒ—ãƒ«
        faq_doc = """# ã‚ˆãã‚ã‚‹è³ªå•ï¼ˆFAQï¼‰

## ã‚·ã‚¹ãƒ†ãƒ åˆ©ç”¨ã«ã¤ã„ã¦

### Q1: ãƒ­ã‚°ã‚¤ãƒ³ã§ãã¾ã›ã‚“
**A:** ä»¥ä¸‹ã®ç‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š
- ãƒ¦ãƒ¼ã‚¶ãƒ¼åãƒ»ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæ­£ã—ã„ã‹
- Caps LockãŒã‚ªãƒ³ã«ãªã£ã¦ã„ãªã„ã‹
- ãƒ–ãƒ©ã‚¦ã‚¶ã®CookieãŒæœ‰åŠ¹ã‹
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šãŒå®‰å®šã—ã¦ã„ã‚‹ã‹

### Q2: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå¤±æ•—ã—ã¾ã™
**A:** ä»¥ä¸‹ã®åˆ¶é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š
- ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: æœ€å¤§100MB
- å¯¾å¿œå½¢å¼: PDF, DOCX, TXT, Markdown
- ãƒ•ã‚¡ã‚¤ãƒ«å: æ—¥æœ¬èªãƒ»ç‰¹æ®Šæ–‡å­—ã¯é¿ã‘ã‚‹
- åŒæ™‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰: æœ€å¤§5ãƒ•ã‚¡ã‚¤ãƒ«

### Q3: æ¤œç´¢çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã›ã‚“
**A:** æ¤œç´¢ã®ã‚³ãƒ„ï¼š
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’2-3èªã«çµã‚‹
- å®Œå…¨ä¸€è‡´ã§ã¯ãªãéƒ¨åˆ†ä¸€è‡´ã§æ¤œç´¢
- é¡ç¾©èªãƒ»é–¢é€£èªã‚‚è©¦ã™
- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚’ç·©ã‚ã‚‹

## æŠ€è¡“çš„ãªå•é¡Œ

### Q4: APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒé…ã„ã§ã™
**A:** ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„æ–¹æ³•ï¼š
- ã‚¯ã‚¨ãƒªã‚’ç°¡æ½”ã«ã™ã‚‹
- ä¸è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’é™¤å¤–
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã‚’æ´»ç”¨
- åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’åˆ¶é™

### Q5: ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰500ãŒç™ºç”Ÿã—ã¾ã™
**A:** ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ã®å¯¾å‡¦æ³•ï¼š
- ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†è©¦è¡Œ
- ãƒ–ãƒ©ã‚¦ã‚¶ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
- ç®¡ç†è€…ã«å•ã„åˆã‚ã›
- ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ç¢ºèª

### Q6: ãƒ‡ãƒ¼ã‚¿ãŒæ›´æ–°ã•ã‚Œã¾ã›ã‚“
**A:** ãƒ‡ãƒ¼ã‚¿åŒæœŸã®ç¢ºèªï¼š
- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°ã‚¿ã‚¤ãƒŸãƒ³ã‚°ï¼ˆ5-10åˆ†ï¼‰
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æœŸé™
- æ¨©é™è¨­å®šã®ç¢ºèª
- æ‰‹å‹•æ›´æ–°ã®å®Ÿè¡Œ

## æ–™é‡‘ãƒ»ãƒ—ãƒ©ãƒ³ã«ã¤ã„ã¦

### Q7: åˆ©ç”¨æ–™é‡‘ã¯ã„ãã‚‰ã§ã™ã‹ï¼Ÿ
**A:** æ–™é‡‘ä½“ç³»ï¼š
- åŸºæœ¬ãƒ—ãƒ©ãƒ³: æœˆé¡Â¥1,000ï¼ˆ100ã‚¯ã‚¨ãƒª/æœˆï¼‰
- ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰: æœˆé¡Â¥5,000ï¼ˆ1,000ã‚¯ã‚¨ãƒª/æœˆï¼‰
- ãƒ—ãƒ¬ãƒŸã‚¢ãƒ : æœˆé¡Â¥20,000ï¼ˆç„¡åˆ¶é™ï¼‰
- å¾“é‡èª²é‡‘: Â¥10/ã‚¯ã‚¨ãƒª

### Q8: ç„¡æ–™ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
**A:** ç„¡æ–™ãƒ—ãƒ©ãƒ³ï¼š
- æœŸé–“: 30æ—¥é–“
- ã‚¯ã‚¨ãƒªæ•°: 50å›/æœˆ
- æ©Ÿèƒ½åˆ¶é™: åŸºæœ¬æ¤œç´¢ã®ã¿
- ã‚µãƒãƒ¼ãƒˆ: ãƒ¡ãƒ¼ãƒ«ã®ã¿
"""

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¿å­˜
        docs = [
            ("azure_ai_overview.md", azure_ai_doc),
            ("python_best_practices.md", python_guide_doc),
            ("system_faq.md", faq_doc)
        ]

        for filename, content in tqdm(docs, desc="æŠ€è¡“æ–‡æ›¸ä½œæˆ"):
            file_path = self.output_dir / "technical_docs" / filename
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)

        logger.info(f"âœ… æŠ€è¡“æ–‡æ›¸ã‚µãƒ³ãƒ—ãƒ« {len(docs)}ä»¶ã‚’ä½œæˆå®Œäº†")

    def download_sample_pdfs(self) -> None:
        """ã‚µãƒ³ãƒ—ãƒ«PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆè»½é‡ï¼‰"""
        logger.info("ã‚µãƒ³ãƒ—ãƒ«PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")

        # è»½é‡ãªã‚µãƒ³ãƒ—ãƒ«PDFã®URL
        pdf_urls = [
            {
                "url": "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf",
                "filename": "w3c_dummy_sample.pdf",
                "description": "W3C ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«PDFï¼ˆè»½é‡ï¼‰"
            }
        ]

        for pdf_info in tqdm(pdf_urls, desc="PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
            try:
                response = requests.get(pdf_info["url"], timeout=30)
                if response.status_code == 200:
                    file_path = self.output_dir / "sample_pdfs" / pdf_info["filename"]
                    with open(file_path, 'wb') as f:
                        f.write(response.content)
                    logger.info(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {pdf_info['filename']} ({len(response.content)} bytes)")
                else:
                    logger.warning(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {pdf_info['url']} (Status: {response.status_code})")
            except Exception as e:
                logger.error(f"âŒ PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ {pdf_info['url']}: {e}")

    def create_test_manifest(self) -> None:
        """ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§ä½œæˆ"""
        logger.info("ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§ã‚’ä½œæˆä¸­...")

        manifest = {
            "created_at": pd.Timestamp.now().isoformat(),
            "total_documents": 0,
            "categories": {
                "wikipedia": {
                    "description": "Wikipediaã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆè»½é‡ç‰ˆï¼‰",
                    "source": "generated_samples",
                    "count": 0,
                    "files": []
                },
                "qa_datasets": {
                    "description": "Q&Aã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæŠ€è¡“ãƒ»ãƒ“ã‚¸ãƒã‚¹é–¢é€£ï¼‰",
                    "source": "generated_samples",
                    "count": 0,
                    "files": []
                },
                "technical_docs": {
                    "description": "æŠ€è¡“æ–‡æ›¸ã‚µãƒ³ãƒ—ãƒ«",
                    "source": "generated",
                    "count": 0,
                    "files": []
                },
                "sample_pdfs": {
                    "description": "ã‚µãƒ³ãƒ—ãƒ«PDFãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè»½é‡ï¼‰",
                    "source": "public_resources",
                    "count": 0,
                    "files": []
                }
            }
        }

        # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        for category in manifest["categories"]:
            category_path = self.output_dir / category
            if category_path.exists():
                files = list(category_path.glob("*"))
                manifest["categories"][category]["count"] = len(files)
                manifest["categories"][category]["files"] = [f.name for f in files]
                manifest["total_documents"] += len(files)

        # ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆä¿å­˜
        manifest_file = self.output_dir / "test_documents_manifest.json"
        with open(manifest_file, 'w', encoding='utf-8') as f:
            json.dump(manifest, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§ä½œæˆå®Œäº†: åˆè¨ˆ {manifest['total_documents']} ãƒ•ã‚¡ã‚¤ãƒ«")

        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\n" + "="*60)
        print("ğŸ“„ ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåé›†å®Œäº†")
        print("="*60)
        for category, info in manifest["categories"].items():
            print(f"ğŸ“ {category}: {info['count']}ä»¶ ({info['description']})")
        print(f"ğŸ“Š åˆè¨ˆ: {manifest['total_documents']}ä»¶")
        print("="*60)

def main():
    parser = argparse.ArgumentParser(description="RAGç”¨ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåé›†ï¼ˆè»½é‡ç‰ˆï¼‰")
    parser.add_argument("--output-dir", default="test_documents", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--wiki-limit", type=int, default=5, help="Wikipediaæ–‡æ›¸æ•°åˆ¶é™")
    parser.add_argument("--squad-limit", type=int, default=3, help="SQuADæ–‡æ›¸æ•°åˆ¶é™")
    parser.add_argument("--skip-download", action="store_true", help="PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    parser.add_argument("--verbose", action="store_true", help="è©³ç´°ãƒ­ã‚°è¡¨ç¤º")

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    collector = TestDocumentCollector(args.output_dir)

    try:
        print("ğŸš€ ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåé›†ã‚’é–‹å§‹...")

        # ãƒ‡ãƒ¼ã‚¿åé›†å®Ÿè¡Œ
        collector.collect_wikipedia_passages(args.wiki_limit)
        collector.collect_squad_dataset(args.squad_limit)
        collector.collect_technical_samples()

        if not args.skip_download:
            collector.download_sample_pdfs()

        # ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆä½œæˆ
        collector.create_test_manifest()

        print(f"\nâœ… ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåé›†å®Œäº†ï¼")
        print(f"ğŸ“ å‡ºåŠ›å…ˆ: {args.output_dir}")
        print(f"ğŸ“‹ è©³ç´°: {args.output_dir}/test_documents_manifest.json")

    except Exception as e:
        logger.error(f"âŒ ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåé›†ã‚¨ãƒ©ãƒ¼: {e}")
        return 1

    return 0

if __name__ == "__main__":
    exit(main())
