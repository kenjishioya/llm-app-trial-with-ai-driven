# API ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè¨­è¨ˆ â€“ FastAPI + Strawberry GraphQL

> **ç›®çš„** â€” ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ API ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆFastAPI **v0.110+**ã€GraphQL ãƒ«ãƒ¼ã‚¿ãƒ¼ã¯ **Strawberry**ï¼‰ã®å†…éƒ¨æ§‹é€ ãƒ»ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ãƒ»ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢æ§‹æˆã‚’æ˜ç¢ºåŒ–ã—ã€ä¿å®ˆãƒ»è¿½åŠ æ©Ÿèƒ½ã®é–‹ç™ºã‚’ã‚¹ãƒ ãƒ¼ã‚ºã«ã™ã‚‹ã€‚

---

## 1. æ¦‚è¦

* **ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ**: `main.py`
* **ASGI ã‚µãƒ¼ãƒ**: `uvicorn` (dev), `gunicorn -k uvicorn.workers.UvicornWorker` (Container App)
* **GraphQL** ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: `/graphql` (HTTP POST) / `/graphql/stream` (SSE over HTTP)
* REST ã¯ `/health`, `/metrics` ã®ã¿ exposeã€‚

## LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æŠ½è±¡åŒ– ğŸ†•

### è¨­è¨ˆåŸå‰‡
- **ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼éä¾å­˜**: ç‰¹å®šã®LLMã‚µãƒ¼ãƒ“ã‚¹ã«ä¾å­˜ã—ãªã„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹è¨­è¨ˆ
- **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½**: ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼éšœå®³æ™‚ã®è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆ
- **è¨­å®šé§†å‹•**: ç’°å¢ƒå¤‰æ•°ãƒ»è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client App    â”‚â”€â”€â”€â–¶â”‚  LLM Service     â”‚â”€â”€â”€â–¶â”‚ Provider Factoryâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â–¼                                 â–¼                                 â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ OpenRouterProviderâ”‚              â”‚ GoogleAIProvider â”‚              â”‚AzureOpenAIProviderâ”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚                                 â”‚                                 â”‚
                       â–¼                                 â–¼                                 â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  OpenRouter API  â”‚              â”‚ Google AI Studio â”‚              â”‚  Azure OpenAI    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©
```python
class ILLMProvider(ABC):
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å…±é€šã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""

    @abstractmethod
    async def chat_completion(
        self,
        messages: List[ChatMessage],
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 2048,
        **kwargs
    ) -> ChatResponse:
        """ãƒãƒ£ãƒƒãƒˆå®Œäº†API"""
        pass

    @abstractmethod
    async def chat_completion_stream(
        self,
        messages: List[ChatMessage],
        **kwargs
    ) -> AsyncGenerator[ChatStreamChunk, None]:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆå®Œäº†API"""
        pass

    @abstractmethod
    async def embedding(
        self,
        text: str,
        model: str = None
    ) -> List[float]:
        """ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿API"""
        pass

    @property
    @abstractmethod
    def provider_name(self) -> str:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å"""
        pass

    @property
    @abstractmethod
    def available_models(self) -> List[str]:
        """åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ä¸€è¦§"""
        pass
```

### ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…ä¾‹
```python
class OpenRouterProvider(ILLMProvider):
    def __init__(self, api_key: str, base_url: str = "https://openrouter.ai/api/v1"):
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self._provider_name = "openrouter"

    async def chat_completion(self, messages: List[ChatMessage], **kwargs) -> ChatResponse:
        response = await self.client.chat.completions.create(
            model=kwargs.get('model', 'deepseek/deepseek-r1:free'),
            messages=[msg.model_dump() for msg in messages],
            **kwargs
        )
        return ChatResponse.from_openai_response(response)

    async def chat_completion_stream(self, messages: List[ChatMessage], **kwargs):
        stream = await self.client.chat.completions.create(
            model=kwargs.get('model', 'deepseek/deepseek-r1:free'),
            messages=[msg.model_dump() for msg in messages],
            stream=True,
            **kwargs
        )
        async for chunk in stream:
            yield ChatStreamChunk.from_openai_chunk(chunk)
```

### ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
```python
class LLMProviderFactory:
    @staticmethod
    def create_provider(provider_name: str, config: Dict) -> ILLMProvider:
        providers = {
            'openrouter': OpenRouterProvider,
            'google_ai': GoogleAIProvider,
            'azure_openai': AzureOpenAIProvider,
        }

        provider_class = providers.get(provider_name)
        if not provider_class:
            raise ValueError(f"Unknown provider: {provider_name}")

        return provider_class(**config)
```

### è¨­å®šç®¡ç†
```yaml
# config/llm.yml
llm:
  primary_provider: "openrouter"
  fallback_providers:
    - "google_ai"
    - "azure_openai"

  providers:
    openrouter:
      api_key: "${OPENROUTER_API_KEY}"
      base_url: "https://openrouter.ai/api/v1"
      default_models:
        chat: "deepseek/deepseek-r1:free"
        embedding: "text-embedding-ada-002"
      retry_config:
        max_retries: 3
        backoff_factor: 2

    google_ai:
      api_key: "${GOOGLE_AI_API_KEY}"
      default_models:
        chat: "gemini-2.5-flash"
        embedding: "text-embedding-004"

    azure_openai:
      api_key: "${AZURE_OPENAI_API_KEY}"
      endpoint: "${AZURE_OPENAI_ENDPOINT}"
      api_version: "2024-02-15-preview"
      default_models:
        chat: "gpt-4o-mini"
        embedding: "text-embedding-ada-002"
```

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚° & ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
```python
class LLMService:
    def __init__(self, config: LLMConfig):
        self.primary_provider = LLMProviderFactory.create_provider(
            config.primary_provider,
            config.providers[config.primary_provider]
        )
        self.fallback_providers = [
            LLMProviderFactory.create_provider(name, config.providers[name])
            for name in config.fallback_providers
        ]

    async def chat_completion(self, messages: List[ChatMessage], **kwargs) -> ChatResponse:
        providers = [self.primary_provider] + self.fallback_providers

        for provider in providers:
            try:
                return await provider.chat_completion(messages, **kwargs)
            except Exception as e:
                logger.warning(f"Provider {provider.provider_name} failed: {e}")
                continue

        raise LLMServiceError("All providers failed")
```

---

## 2. ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆï¼ˆ`backend/`ï¼‰

```text
backend/
  main.py                # FastAPI app factory
  api/
    graphql_schema.py    # Strawberry type defs & resolvers
    middleware.py        # CORS, Auth, RateLimit
    deps.py              # Depends() å®šç¾©
  services/
    rag.py               # RagService
    deep_agent.py        # DeepResearchAgent
  infra/
    search_client.py     # Azure AI Search helper
    openai_client.py     # Azure OpenAI helper
    db.py                # SQLAlchemy async engine
  config.py              # Pydantic BaseSettings
```

---

## 3. ã‚¯ãƒ©ã‚¹å›³ï¼ˆHighâ€‘levelï¼‰

```mermaid
classDiagram
  class ApiApp {
    +FastAPI app
    +lifespan()
  }
  class GraphQLRouter {
    +schema: strawberry.Schema
    +graphql_app: GraphQL
  }
  class RagService {
    +answer(question) : AsyncIterator[str]
  }
  class DeepResearchAgent {
    +run(question) : AsyncIterator[str]
  }
  ApiApp --> GraphQLRouter : include_router
  GraphQLRouter --> RagService : call
  GraphQLRouter --> DeepResearchAgent : call
```

---

## 4. ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã‚¹ã‚¿ãƒƒã‚¯

| é †åº | ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢                        | å½¹å‰²                        | å®Ÿè£…ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸               |
| -- | ----------------------------- | ------------------------- | --------------------- |
| 1  | `structlog` Logging           | JSON æ§‹é€ ãƒ­ã‚°å‡ºåŠ›ãƒ»trace\_id ä»˜ä¸  | `structlog`, `loguru` |
| 2  | `TrustedHost`                 | Host ãƒ˜ãƒƒãƒ€ãƒã‚§ãƒƒã‚¯              | FastAPI builtâ€‘in      |
| 3  | `CORSMiddleware`              | `*.staticapps.net` ã®ã¿è¨±å¯   | FastAPI builtâ€‘in      |
| 4  | **AuthMiddleware (optional)** | Azure AD OIDC ãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼      | `pythonâ€‘jose`, `msal` |
| 5  | **RateLimitMiddleware**       | 20 req/min/IP (free tier) | `slowapi`             |
| 6  | `GZipMiddleware`              | 1 KiB ä»¥ä¸Šãƒ¬ã‚¹ãƒãƒ³ã‚¹åœ§ç¸®           | FastAPI builtâ€‘in      |

> Dev ç’°å¢ƒã§ã¯ Auth ã‚’ã‚¹ã‚­ãƒƒãƒ—å¯ã€‚RateLimit ã®è¨­å®šã«ã¤ã„ã¦ã¯ **[../environment_setup.md](../environment_setup.md)** ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---

## 5. Lifespan & Dependency Injection

```python
@app.on_event("startup")
async def startup():
    await db.init_async_engine()
    search_client = init_ai_search()
    openai_client = init_openai()
    app.state.clients = {"search": search_client, "openai": openai_client}
```

* **ä¾å­˜æ³¨å…¥**: `Depends(get_clients)` ã§ Service å±¤ã«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå…±æœ‰ã€‚
* **shutdown**: ã‚¨ãƒ³ã‚¸ãƒ³ disposeï¼HTTP ã‚»ãƒƒã‚·ãƒ§ãƒ³ closeã€‚

---

## 6. GraphQL Resolver ãƒãƒªã‚·ãƒ¼

| ç¨®åˆ¥           | åå‰             | ãƒãƒ³ãƒ‰ãƒ©ãƒ¼                       | è¿”å´å‹                   |
| ------------ | -------------- | --------------------------- | --------------------- |
| Query        | `sessions`     | `get_sessions()`            | `[Session]`           |
| Mutation     | `ask`          | `ask_rag()` or `ask_deep()` | `AskPayload`          |
| Subscription | `streamAnswer` | `stream_answer()`           | `AsyncGenerator[str]` |

`ask_mutation` ã¯ `deepResearch` å¼•æ•°ã§ãƒ«ãƒ¼ãƒˆåˆ‡æ›¿ã€‚ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¯ Strawberry ã® `@strawberry.subscription` + `graphqlâ€‘sse` ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã€‚

---

## 7. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

åŒ…æ‹¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥ã«ã¤ã„ã¦ã¯ **[error_handling.md](error_handling.md)** ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

API ãƒ¬ã‚¤ãƒ¤ã§ã®åŸºæœ¬çš„ãªä¾‹å¤–ã‚­ãƒ£ãƒƒãƒã¨ GraphQL ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹å¤‰æ›ã®ã¿ã‚’æ‹…å½“ã—ã€å…·ä½“çš„ãªãƒªãƒˆãƒ©ã‚¤ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥ã¯å„ã‚µãƒ¼ãƒ“ã‚¹å±¤ã§å®Ÿè£…ã—ã¾ã™ã€‚

---

## 8. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡é‡

* **Async** ã™ã¹ã¦ã®å¤–éƒ¨ I/O (`httpx.AsyncClient`, `asyncpg`)ã€‚
* **Worker æ•°**: `uvicorn --workers $(nproc)` (Container App scales)ã€‚
* **Caching**: Inâ€‘process LRU 256 entries for embedding vector cache (TODO redis option)ã€‚

---

## 9. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ TODOï¼ˆprodï¼‰

* **mTLS** between Container App and AI Search via Private Endpointã€‚
* **GraphQL Depth Limit** 10, Complexity Limit 1000 (plugin)ã€‚

## 10. ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

è©³ç´°ãªãƒ†ã‚¹ãƒˆæˆ¦ç•¥ï¼ˆãƒ¦ãƒ‹ãƒƒãƒˆãƒ»çµ±åˆãƒ»E2Eãƒ»è² è·ãƒ†ã‚¹ãƒˆï¼‰ã«ã¤ã„ã¦ã¯ **[test_strategy.md](test_strategy.md)** ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

API ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«é–¢é€£ã™ã‚‹ä¸»è¦ãƒ†ã‚¹ãƒˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼š

| ãƒ¬ã‚¤ãƒ¤            | ãƒ„ãƒ¼ãƒ«                               | ç›®çš„                                                                                 |
| -------------- | --------------------------------- | ---------------------------------------------------------------------------------- |
| **ãƒ¦ãƒ‹ãƒƒãƒˆ / ãƒ«ãƒ¼ã‚¿** | `pytest` + **FastAPI TestClient** | ä¾å­˜ã‚’ãƒ¢ãƒƒã‚¯ã—ã€RAGãƒ»DeepResearchResolver ãŒæœŸå¾… JSON ã‚’è¿”ã™ã‹ 50 ms ä»¥å†…ã§æ¤œè¨¼                         |
| **è² è·ãƒ»å›ç·šã‚³ã‚¹ãƒˆ**   | **Locust** (Python)               | ç„¡æ–™æ  Container App ã® QPS ä¸Šé™ â‰ˆ 20 RPS ã‚’è¶Šãˆãªã„ã‹ã‚·ãƒŠãƒªã‚ªè©¦é¨“ã€‚ã‚·ãƒŠãƒªã‚ªä¾‹: 5 ãƒ¦ãƒ¼ã‚¶åŒæ™‚ / 1 msg sâ»Â¹ ã§ 5 åˆ†é–“ |

```
